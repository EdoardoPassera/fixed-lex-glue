"eval_macro-f1": 0.7105927401681831,
    "eval_micro-f1": 0.7107692307692307,
    "eval_runtime": 1051.9393,

					 VALIDATION                                      | TEST
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                  bert-base-uncased: MICRO-F1: 71.1	 ± 0.0	MACRO-F1: 71.1	 ± 0.0	 | MICRO-F1: 68.1	MACRO-F1: 68.1	
                       roberta-base: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan	
             microsoft/deberta-base: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan	
       allenai/longformer-base-4096: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan	
        google/bigbird-roberta-base: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan	
    nlpaueb/legal-bert-base-uncased: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan	
            zlucia/custom-legalbert: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan	
                      roberta-large: MICRO-F1: nan	 ± nan	MACRO-F1: nan	 ± nan	 | MICRO-F1: nan	MACRO-F1: nan
   nlpaueb/legal-bert-small-uncased: MICRO-F1: 72.6	 ± 0.0	MACRO-F1: 72.6   ± 0.0	 | MICRO-F1: 71.0	MACRO-F1: 71.0